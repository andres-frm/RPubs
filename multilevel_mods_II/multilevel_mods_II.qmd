---
title: "Introducción modelos jerárquicos (II)"
format: 
  html:
    theme: 
      light: journal
      dark: [journal, theme-dark.scss]
    toc: true
    toc-depth: 10
    toc-expand: 10
    toc-title: "Tabla de contenido"
    toc-location: left
    embed-resources: true
number-sections: true
number-depth: 10
editor: visual
date-format: full 
date-modified: now
mainfont: Times New Roman
code-fold: false
code-overflow: scroll
code-line-numbers: true
code-copy: true
---

```{r, echo=TRUE, results='hide',  warning = F, message = F}

knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)

paquetes <- c("rethinking", "tidyverse", "magrittr", 'patchwork',
              'cmdstanr', 'loo', "MASS", 'ellipse')

sapply(paquetes, library, character.only = T)

options(mc.cores = parallel::detectCores())

source('functions_mod_diagnostics.R')

```

En el [capítulo anterior](https://rpubs.com/andresfeli/multilevel_I) expuse cómo los modelos jerárquicos usan la información disponible en cada agrupamiento (*cluster*) de datos para generar estimaciones más precisas de los parámetros (*partial pooling*). Presenté las ventajas de esta aproximación, y su utilidad cuando trabajamos con observaciones desbalanceadas entre grupos. usar esta aproximación suponía considerar distribuciones de probabilidad de la población de parámetros (*previa adaptativa*), hiper previas (*distribuciones previas de las previas*) y la reparametrización de los modelos para para mejorar la eficiencia computacional y la convergencia de las cadenas. Bien, en esta sección aumentaremos el nivel de abstracción. Emplearemos *partial pooling* para cruzar la información ya no solo entre *clusters*, sino también entre parámetros. Este método aprovecha la covariación entre parámetros para extraer una mayor cantidad de información de los datos, y es la base para extender los modelos jerárquicos a áreas más especializadas (e.g. modelos filogenéticos, modelos espacio-estado, análisis de redes).

::: {.border}

El material de esta sección es avanzado, requiere la revisión del [capítulo anterior](https://rpubs.com/andresfeli/multilevel_I), algunas nociones adicionales de [programación en `Stan`]() y mucha paciencia. Si usted es una persona con capacidades intelectuales convencionales, igual que yo, el entendimiento y la intuición para ajustar este tipo de modelos solo ocurrirá con la repetición y práctica.

:::


# Parámetros correlacionados y distribución normal multivariada

Acabo de mencionar que este tipo de modelos aprovecha la covariación entre parámetros para intercambiar información y producir mejores estimaciones. En este caso, el modelo emplea un *vector de promedios* para cada parámetros --cada promedio es en realidad una distribución de probabilidad-- y una *matriz de varianza-covarianza* para parametrizar una *distribución normal multivariada* que asocia los parámetros y realizar su estimación. Veamos en qué consiste:

Supongamos los parametros $\theta \sim normal(\mu=2, \sigma = 1.2)$ y $\psi \sim normal(\mu = 1.1, \sigma=0.8)$, cuyo coeficiente de correlación $r = 0.7$. Para generar la distribución normal multivariada que relaciona ambos parámetros requerimos:

i. Vector de promedios $\mu = [\mu_\theta, \mu_\psi]$:
```{r}
theta <- 2
psi <- 1.1
mu <- c(theta, psi)
```

ii. Matriz de varianza-covarianza. Esta, es una matriz cuadrada cuya diagonal corresponde a varianza de los parámetros y mientras (i.e. $\sigma^2_\theta$, $\sigma^2_\psi$) los demás elementos de la matriz corresponden a la covariación $\rho_{\theta \psi}$ entre ambos parámetros. Bien, la covarianza entre ambos parámetros se calcula a partir de la fórmula: $COV(\theta, \psi) = \sigma^2_\theta \times \sigma^2_\psi \times \rho_{\theta \psi}$. Calculemos $COV$ y definamos nuestra matriz de varianza-covarianza:

```{r}
sigma_theta <- 1.2
sigma_psi <- 0.8
r <- 0.7
cov_theta_psi <- sigma_theta * sigma_psi * r

cov_m <- matrix(c(sigma_theta^2, cov_theta_psi, cov_theta_psi, sigma_psi^2), 
                ncol = 2) # tantas columnas como parámetros
cov_m
```

Tenemos los elementos necesarios para parametrizar la distribución normal multivariada que describe $\theta$ y $\psi$. Usaremos $MASS::mvrnorm()$ para generar valores aleatorios de la distribución:

```{r}

multi_norm <- mvrnorm(1e3, mu = mu, Sigma = cov_m)

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
plot(density(multi_norm[, 1]), main = '', xlab = expression(theta),
     lwd = 2, col = 2)
lines(density(rnorm(1e3, theta, sigma_theta)),  
     lwd = 2, col = 3)
plot(density(multi_norm[, 2]), main = '', xlab = expression(psi), 
     lwd = 3, col = 2)
lines(density(rnorm(1e3, psi, sigma_psi)),  
     lwd = 3, col = 3)

par(mfrow = c(1, 1))

```
$MASS::mvrnorm()$ genera una distribución con dos normal con dos dimensiones, una por parámetro (Figura anterior). Grafiquemos la distribución multivariada:

```{r}
plot(multi_norm[, 1], multi_norm[, 2], main = '', 
     ylab = expression(theta), xlab = expression(psi))
for (i in seq(0.1, 0.9, by = 0.2)) {
  lines(ellipse(cov_m, centre = mu, level = i), col = 4, lwd = 1.5)
}
```

Será justamente este tipo de distribución normal multivariada el que usaremos como distribución previa en nuestros modelos. Ajustarlos requerirá algunas consideraciones adicionales respecto a la parametrización y reparametrización de las distribuciones previas, operaciones con matrices y algunos elementos nuevos de programación en `Stan`. 

veamos un ejemplo:


# Ejemplo: datos sintéticos de depredación de huevos
Para contextualizar el ajuste de est tipo de modelos generemos un conjuntos de datos sintéticos sobre depredación de nidadas de el ave `X`. Supongamos que una investigadora y su equipo monitorearon 1000 nidadas con `N` huevos cada 1; los huevos estuvieron distribuidos en 20 parches de bosque y bajo dos condiciones de estrato vegetal, arbusto y arbóreo. Generemos los datos:

```{r}

```



```{r eval=FALSE}
data(bangladesh)
d <- bangladesh

d$district_id <- as.integer(as.factor(d$district))

colnames(d)

dat <- 
  list(
    anticonc = d$use.contraception,
    urbano = as.integer(ifelse(d$urban == 1, 1, 2)),
    distrito = d$district_id,
    edad = d$age.centered,
    N = nrow(d),
    N_distrito = length(unique(d$district_id)),
    N_urbano = 2
  )

cat(file = 'multilevel_mods_II/corr_par1.stan', 
    '
    data{
      int N;
      int N_distrito;
      int N_urbano;
      array[N] int anticonc;
      array[N] int distrito;
      array[N] int urbano;
      //vector[N] edad;
    }
    
    parameters{
      matrix[N_distrito, N_urbano] v;
      vector[2] alpha_bar;
      corr_matrix[2] Rho;
      vector<lower = 0>[2] sigma;
    }
    
    transformed parameters {
      vector[N_distrito] alpha;
      vector[N_distrito] beta;
      alpha = v[, 1];
      beta = v[, 2];
    }
    
    model{
      vector[N] p;
      alpha_bar ~ normal(0, 1);
      Rho ~ lkj_corr(4);
      sigma ~ exponential(1);
      
      for (i in 1:N_distrito) v[i,:] ~ multi_normal(alpha_bar, quad_form_diag(Rho, sigma));
      
      for (i in 1:N) {
        p[i] = alpha[distrito[i]] + beta[distrito[i]]*urbano[i]; 
        p[i] = inv_logit(p[i]);
      }
    
      anticonc ~ binomial(1, p);
    
    }
    
    generated quantities{
      vector[N] log_lik;
      vector[N] prob;
      array[N] int sim;

      for (i in 1:N) {
        prob[i] = alpha[distrito[i]] + beta[distrito[i]]*urbano[i];
        prob[i] = inv_logit(prob[i]);
      }

      for (i in 1:N) log_lik[i] = binomial_lpmf(anticonc[i] | 1, prob[i]);
    
      sim = binomial_rng(1, prob);
}
    ')


file <- paste(getwd(), '/multilevel_mods_II/corr_par1.stan', sep = '')

fit_corr_par1 <- cmdstan_model(file, compile = T)

m1 <- 
  fit_corr_par1$sample(
    data = dat,
    iter_sampling = 4e3,
    iter_warmup = 500,
    parallel_chains = 3, 
    chains = 3,
    thin = 3,
    seed = 123,
    refresh = 500
  )


(out_m1 <- m1$summary())
out_m1 |> print(n = 249)

par(mfrow = c(3, 3), mar = c(4, 4, 1, 1))
for (i in 1:9) trace_plot(m1, out_m1$variable[i], 3)
par(mfrow = c(1, 1))

mod_diagnostics(m1, out_m1)



cat(file = "multilevel_mods_II/corr_par2.stan", 
    "
    data{
      int N;
      int N_distrito;
      int N_urbano;
      array[N] int anticonc;
      array[N] int distrito;
      array[N] int urbano;
      //vector[N] edad;
    }
    
    parameters{
      matrix[N_urbano, N_distrito] Z;
      vector[2] alpha_bar;
      cholesky_factor_corr[2] L_Rho;
      vector<lower = 0>[2] sigma;
    }
    
    transformed parameters {
      vector[N_distrito] alpha;
      vector[N_distrito] beta;
      matrix[N_distrito, N_urbano] v;
      v = (diag_pre_multiply(sigma, L_Rho) * Z)';
      alpha = alpha_bar[1] + v[, 1];
      beta = alpha_bar[2] + v[, 2];
    }
    
    model{
      vector[N] p;
      alpha_bar ~ normal(0, 1);
      sigma ~ exponential(1);
      L_Rho ~ lkj_corr_cholesky(4);
      to_vector(Z) ~ normal(0, 1);
      
      for (i in 1:N) {
        p[i] = alpha[distrito[i]] + beta[distrito[i]]*urbano[i];
        p[i] = inv_logit(p[i]);
      }
      
      anticonc ~ binomial(1, p);
    }
    
    generated quantities{
      vector[N] log_lik;
      array[N] int sim;
      vector[N] p;
      matrix[2, 2] Rho;
      Rho = multiply_lower_tri_self_transpose(L_Rho);
      
      for (i in 1:N) {
        p[i] = alpha[distrito[i]] + beta[distrito[i]]*urbano[i];
        p[i] = inv_logit(p[i]);
      }
      
      for (i in 1:N) log_lik[i] = binomial_lpmf(anticonc[i] | 1, p[i]);
    
      sim = binomial_rng(1, p);
    }
    ")

file <- paste(getwd(), '/multilevel_mods_II/corr_par2.stan', sep = '')

fit_m2 <- cmdstan_model(file, compile = T)

m2 <- 
  fit_m2$sample(
    data = dat,
    chains = 3,
    parallel_chains = 3, 
    iter_sampling = 1e4,
    iter_warmup = 500,
    thin = 3,
    refresh = 500, 
    seed = 123
  )

out_m2 <- m2$summary()

par(mfrow = c(3, 3), mar = c(4, 4, 1, 1))
for (i in 1:9) trace_plot(m2, out_m2$variable[i], 3)
par(mfrow = c(1, 1))

mod_diagnostics(m2, out_m2)


```
